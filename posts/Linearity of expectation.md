The **expected value of a constant** is the constant itself. $𝐸(𝑐)=𝑐 ; 𝑐$ is any constant. This can be verified by noting that:

$$𝐸(𝑐)=\int_{−∞}^∞𝑐𝑓(𝑥)𝑑𝑥=𝑐\int_{−∞}^∞𝑓(𝑥)𝑑𝑥$$

and by definition:

$$\int_{−∞}^∞𝑓(𝑥)𝑑𝑥=1$$

Therefore, the expected value of a constant times a random variable is the constant times the expected value of the random variable:

$$𝐸[𝑐𝑋]=𝑐𝐸[𝑋]$$

where 𝑐 is any constant with respect to 𝑋.

This can also be verified by:

$$𝐸[𝑐𝑋]=\int_{−∞}^∞𝑐𝑥𝑓(𝑥)𝑑𝑥=𝑐\int_{−∞}^∞𝑥𝑓(𝑥)𝑑𝑥=𝑐𝐸[𝑋]$$

The expected value of two terms is the sum of the expected value of each.

$$𝐸[𝑋+𝑌]=𝐸[𝑋]+𝐸[𝑌]$$

$$𝐸[𝑓(𝑥)+𝑔(𝑥)]=𝐸[𝑓(𝑥)]+𝐸[𝑔(𝑥)]$$

The expected value of a random variable 𝑋 is also called the **mean of 𝑋** and is often designated by μ. The expected value of $(𝑋–μ)^2$ is called the variance of 𝑋. The positive square root of the variance is called the standard deviation. The terms $σ^2$ and $σ$ (sigma squared and sigma) represent the variance and standard deviation, respectively. **Variance** is a measure of the spread or dispersion of the values of the random variable about its mean value. 

The **standard deviation** is $\sqrt{𝑉[𝑋]}$, which is also a measure of spread or dispersion. The standard deviation is expressed in the same units as 𝑋, whereas the variance is expressed in the square of these units.