If we have two random variables 𝑋 and 𝑌 we can define the covariance of two random variables 𝑋 and 𝑌, $𝐶𝑜𝑣(𝑋,𝑌)$ as:

$$𝑐𝑜𝑣(𝑋,𝑌)=𝐸[(𝑋−𝑥)(𝑌−𝑦)]$$

where μx and μy are the means of 𝑋 and Y, respectively. The above can be written as:

$$𝑐𝑜𝑣(𝑋,𝑌)=𝐸[(𝑋−𝐸[𝑋])(𝑌−𝐸[𝑌])]=𝐸[𝑋𝑌]−𝐸[𝑋]𝐸[𝑌]$$

From its definition, we see that covariance satisfies the following properties:

$$𝐶𝑜𝑣(𝑋,𝑌)=𝐶𝑜𝑣(𝑌,𝑋)$$

and

$$𝐶𝑜𝑣(𝑋,𝑋)=𝑉𝑎𝑟(𝑋)$$

Another property of covariance, which immediately follows from its definition, is that for any constant 𝑎:

$$𝐶𝑜𝑣(𝑎𝑋,𝑌)=𝑎𝐶𝑜𝑣(𝑋,𝑌)$$

if 𝑋 and 𝑌 are independent random variables, then

$$𝐶𝑜𝑣(𝑋,𝑌)=0$$

and so for independent $𝑋1,...,𝑋𝑛$

$$𝑉𝑎𝑟(\sum_{𝑖=1}^𝑛𝑋𝑖)=\sum_{𝑖=1}^𝑛𝑉𝑎𝑟(𝑋𝑖)$$