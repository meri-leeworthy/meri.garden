1. **Matrices**: Rectangular arrays of numbers arranged in rows and columns. Denoted by capital letters (e.g., A, B).
2. **[[Matrix Addition]]**: Adding corresponding elements of two matrices of the same dimensions to form a new matrix.
3. **Scalar Multiplication**: Multiplying every element of a matrix by a scalar value.
4. **[[Matrix Multiplication]]**: Combining two matrices by taking the dot product of rows and columns. Only possible if the number of columns in the first matrix equals the number of rows in the second matrix.
5. **Identity Matrix**: Square matrix with 1s on the diagonal and 0s elsewhere. Denoted as $I$. Acts as the multiplicative identity in matrix multiplication.
6. **Transpose**: Flipping a matrix over its diagonal, turning rows into columns and vice versa. Denoted as $A^T$.
7. **[[Determinant]]**: Scalar value that can be computed from a square matrix, indicating whether a matrix is invertible. Denoted as $det‚Å°(A)$.
8. [[Cofactor Matrix]]
9. [[Adjugate (Adjoint) Matrix]]:
10. **[[Inverse Matrix]]**: Matrix $A^{-1}$ that, when multiplied by matrix $A$, yields the identity matrix. Only square matrices with non-zero determinants have inverses.
11. **Rank**: Number of linearly independent rows or columns in a matrix. Indicates the dimension of the vector space spanned by the matrix.
12. **[[Eigenvalues and Eigenvectors]]**: For a square matrix $A$, an eigenvector is a non-zero vector $v$ such that $Av = \lambda v$, where $\lambda$ is the eigenvalue. Indicates directions of scaling.
13. **[[Diagonalization]]**: Decomposing a matrix into a product of its eigenvectors and eigenvalues. A matrix $A$ is diagonalizable if $A = PDP^{-1}$, where $D$ is a diagonal matrix.
14. **Trace**: Sum of the diagonal elements of a square matrix. Denoted as $\text{tr}(A)$.
15. **Orthogonal Matrices**: Square matrices whose rows and columns are orthonormal vectors. For an orthogonal matrix $Q$, $Q^TQ=I$.
16. **Singular Value Decomposition (SVD)**: Factorization of a matrix into three matrices $U$, $\Sigma$, and $V^T$, where $U$ and $V$ are orthogonal matrices, and $\Sigma$ is a diagonal matrix of singular values.

Understanding these concepts forms the foundation of matrix algebra and is crucial for applications in linear transformations, systems of linear equations, and various fields of science and engineering.